---
title: "Coverage regression"
output: html_document
date: "2022-12-02"
---

# Fitting the Coverage Regression Model

```{r}
library(rstudioapi)
library(haven)
library(dplyr)
library(ggplot2)
library(readr)
library(measurements)
library(geosphere)
library(usmap)
```

```{r}
#running this should automatically configure the filepath
path <- paste0(dirname(getSourceEditorContext()$path), '/data/raw/')
setwd(path)

# load all relevant datasets
commtowers <- read_dta('commtowers.dta')
coverage <- read_dta('coverage.dta')

# Find relevant columns in commtowers
is_uhf <- as.integer(commtowers$channel > 13) # 0 is VHF, 1 is UHF
is_uhf

# We want Visual Power to be in increments of 100 kW.
# I think power_vis column is in kW, so divide by 100.
kw_100 <- commtowers$power_vis / 100

# We want height above ground to be measured in increments of 100 feet
# hgtground column is in feet, so divide by 100
hgt_100 <- commtowers$hgtground / 100

tower_df_unclean <- data.frame(is_uhf, kw_100, hgt_100, station=commtowers$station)
tower_df <- na.omit(tower_df_unclean)

#remove satellite stations from consideration
coverage <- coverage[coverage$satellite == 0,]

# Calculate distance from county centroid to channel's tower
covrate_linearized <- coverage$covrate
covrate_linearized[covrate_linearized=="5-24%"] <- 20
covrate_linearized[covrate_linearized=="25-50%"] <- 40
covrate_linearized[covrate_linearized=="over 50%"] <- 90
covrate_linearized[covrate_linearized=="."] <- NA

dists <- sapply(1:nrow(coverage), function(i) {
  # Get <station, county> pair
  stn <- coverage$station[i]
  county_lat <- coverage$lathh[i]
  county_long <- coverage$longhh[i]
  if (stn %in% commtowers$station) {
    # compute distance from that county's population centroid to the station's tower
    tower_info <- commtowers[commtowers$station==stn,]
    dist <- distm(c(county_long, county_lat), 
          c(tower_info$longtower, tower_info$lattower), fun = distHaversine)
    
    # Convert meters to miles
    dist_mi <- conv_unit(dist, "m", "mile")
    return(dist_mi)
  } 
  return(NA)
})

dists[dists>=200] <- NA
# Each row is a <county, station> pair, with its distance to the broadcasting tower
cov_df <- na.omit(data.frame(station=coverage$station, dists_10 = dists/10, 
                             covrate_linearized,
                             state=coverage$state_hh, county=coverage$county_hh, 
                             totalhh = coverage$totalhh))
# Join with info on the tower
cov_df <- inner_join(x=cov_df, y=tower_df, by = 'station')

#sum(is.na(cov_df)) # no na's

coverage.lm <- lm(covrate_linearized ~ dists_10 + is_uhf + kw_100 + hgt_100 
                  + dists_10*is_uhf, data=cov_df, weights=totalhh)

coef(coverage.lm)
```
## Extra: Fitting Different Coverage Regression Model

Something different and fun I want to try: what if we did a logistic regression,
and disaggregated the data so that each observation is a TV household in a county-station pairing,
and the response variable is now whether or not that TV household in that county can receive the station.
We have coverage rates for all TV households in each county-station pairing already, and the number of TV households in the county, so we can just multiply to get the number of TV households that get the station, and disaggregate so that
a proportion (the coverage rate) of TV households get '1' as the response and everyone else gets '0'.
Who gets '1' and who gets '0' doesn't matter, since there are no other household-level variables to worry
about (everyone in a county-station pairing has the same covariates).

```{r}
# extra: logistic regression model
can_watch <- round(((as.integer(cov_df$covrate_linearized))/100)*cov_df$totalhh)
cant_watch <- cov_df$totalhh - can_watch
split <- matrix(c(can_watch, cant_watch), ncol = 2)

logistic_model <- glm(split ~ dists_10 + is_uhf + kw_100 + hgt_100 + dists_10*is_uhf, family='binomial', data=cov_df)

logistic_model
```

# Using Coverage Regression Model to Predict Coverage Rate of SS Stations

```{r}
path <- paste0(dirname(getSourceEditorContext()$path), '/data/raw/')
setwd(path)

SSstations <- read_dta('SSstations.dta')
pbs_towers <- read_dta('pbstowers.dta')
counties <- read_dta('counties.dta')
```

```{r}
# match PBS towers with the stations that air Sesame Street
SStowers <- inner_join(pbs_towers, SSstations, by = 'station')
# get all possible station and county pairs 
crossprod <- full_join(SStowers, counties, by = character())
# grab the station and county coordinates
station_locs <- matrix(c(crossprod$longtower, crossprod$lattower), ncol = 2)
county_locs <- matrix(c(crossprod$longhh, crossprod$lathh), ncol = 2)
# compute distances
dist <- distHaversine(p1=station_locs, p2=county_locs)
dist_mi <- conv_unit(dist, "m", "mile")
# add back to the crossproduct, and filter for <= 200 mi
crossprod <- crossprod %>% mutate(dist_mi = dist_mi)
SSstation_cty_pairs <- crossprod %>% filter(dist_mi <= 200)
# add the UHF indicator
SSstation_cty_pairs <- SSstation_cty_pairs %>% mutate(is_uhf = as.integer(SSstation_cty_pairs$channel.x > 13))
# grab columns that are of interest to the predictions
SSstation_cty_pairs <- SSstation_cty_pairs %>% select(power_vis, hgtground, dist_mi, is_uhf, ctyname, stname, ctyfips_hh, stfips_hh, pop70)
# drop any observations for which there is a missing value of any of these crucial measures for coverage prediction
SSstation_cty_pairs <- na.omit(SSstation_cty_pairs)
# note: there are no county-station pairings left with missing values in these columns

# do unit conversions before feeding into fitted regression
SSstation_cty_pairs <- SSstation_cty_pairs %>% mutate(dists_10 = dist_mi/10, kw_100 = power_vis/100, hgt_100 = hgtground/100)

# get coverage predictions
SScov_preds <- predict.lm(coverage.lm, SSstation_cty_pairs)
SSstation_cty_covs <- SSstation_cty_pairs %>% mutate(pred_coverage = SScov_preds)
SSstation_cty_covs$ctyfips_hh <- sprintf("%03s", SSstation_cty_covs$ctyfips_hh)
SSstation_cty_covs$stfips_hh <- sprintf("%02s", SSstation_cty_covs$stfips_hh)

# take maximum coverage for each county
SSstation_cty_final <- SSstation_cty_covs %>% group_by(ctyfips_hh, stfips_hh) %>% mutate(max_pred_cov = max(pred_coverage)) %>% filter(pred_coverage == max_pred_cov) %>% ungroup() %>% select(-pred_coverage)
```

```{r}
# displaying and saving the final data frame
out_path <- paste0(dirname(getSourceEditorContext()$path), '/data/processed/')
SSstation_cty_final
saveRDS(SSstation_cty_final, file=paste0(out_path, 'SScoverage.Rda'))
```

```{r}
# checking that the national coverage rate is roughly what we would expect, they got 65%
sum(SSstation_cty_final$pop70*(SSstation_cty_final$max_pred_cov/100))/sum(SSstation_cty_final$pop70)
# checks out!
```

Now, let's make a plot to compare the simulated coverage rates by county we get with
their Figure 2.

```{r}
# join county and state fips codes into single codes for plotting
cty_fips <- SSstation_cty_final$ctyfips_hh
st_fips <- SSstation_cty_final$stfips_hh

st_fips <- sprintf("%02s",st_fips)
cty_fips <- sprintf("%03s",cty_fips)
fips_tot <- paste0(st_fips, cty_fips)

# discretize the predicted coverage variable
plt_df_orig <- data.frame(fips=fips_tot, pred_coverage=SSstation_cty_final$max_pred_cov)
discrete <- rep(0, nrow(plt_df))
discrete[plt_df_orig$pred_coverage < 50] = 1
discrete[plt_df_orig$pred_coverage >= 50 & plt_df_orig$pred_coverage < 60] = 2
discrete[plt_df_orig$pred_coverage >= 60 & plt_df_orig$pred_coverage < 70] = 3
discrete[plt_df_orig$pred_coverage >= 70 & plt_df_orig$pred_coverage < 80] = 4
discrete[plt_df_orig$pred_coverage >= 80] = 5
plt_df_orig <- plt_df_orig %>% mutate(pred_cov_discrete = as.factor(discrete))

# plot
plot_usmap(data=plt_df_orig, values='pred_cov_discrete', size = 0.1) + 
  scale_fill_brewer(name = 'Coverage', labels =c('< 50%', '50-60%', 
                                                 '60-70%', '70-80%', '>80%', 'No Data'))
```

A few notes about this process:

+ It seems like there's a few stations where the channel in `pbs_towers`
is different from the channel in `SSstations` (KTEH, KUON, WKSO). However, in all cases,
the mismatched channels would both either be classified as UHF or VHF, so there are 
no ambiguous cases.

+ Some of the counties have negative Sesame Street coverage, but it doesn't seem like the authors address this. Maybe a better reason to use a logistic regression instead, since in using a normal linear regression, you are not guaranteed to get coverages between 0 and 100 percent.

## Extra: Getting SS coverage predictions using alternative logistic regression model

```{r}
# get coverage predictions
SScov_logistic_preds <- predict(logistic_model, SSstation_cty_pairs, type = 'response')
SSstation_logistic <- SSstation_cty_pairs %>% mutate(pred_coverage = SScov_logistic_preds)

# take maximum coverage for each county
SSstation_logistic_final <- SSstation_logistic %>% group_by(ctyfips_hh, stfips_hh) %>% mutate(max_pred_cov = max(pred_coverage)) %>% filter(pred_coverage == max_pred_cov) %>% ungroup() %>% select(-pred_coverage)
```

```{r}
# even using logistic regression, we get 63% national coverage
# very similar to their results
sum(SSstation_logistic_final$pop70*(SSstation_logistic_final$max_pred_cov))/sum(SSstation_logistic_final$pop70)
```

Making a map to compare to Figure 2, using predictions from the logistic model.
Hmm, something's wrong here, let's come back to it..

```{r}
# discretize the predicted coverage variable
plt_df_log <- data.frame(fips=fips_tot, pred_coverage=SSstation_logistic_final$max_pred_cov*100)
discrete <- rep(0, nrow(plt_df))
discrete[plt_df_log$pred_coverage < 50] = 1
discrete[plt_df_log$pred_coverage >= 50 & plt_df_log$pred_coverage < 60] = 2
discrete[plt_df_log$pred_coverage >= 60 & plt_df_log$pred_coverage < 70] = 3
discrete[plt_df_log$pred_coverage >= 70 & plt_df_log$pred_coverage < 80] = 4
discrete[plt_df_log$pred_coverage >= 80] = 5
plt_df_log <- plt_df_log %>% mutate(pred_cov_discrete = as.factor(discrete))

# plot
plot_usmap(data=plt_df_log, values='pred_cov_discrete', size = 0.1) + scale_fill_brewer(name = 'Coverage', labels =c('< 50%', '50-60%', '60-70%', '70-80%', '>80%', 'No Data'))
```
Notes:
+ We would probably argue that logistic regression is a better approach for the reasons described above
+ However, the map looks pretty bad. Maybe a bug where county ID's and their values are mismatched. Should come back to this if we have time.

